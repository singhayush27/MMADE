# -*- coding: utf-8 -*-
"""Fine-tune_Blip.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ss0V4Mbd2r6adbwXviYiSdHfiMK8rJA8
"""

# Install latest version of the library
!pip install -q datasets==2.13.1

pip install transformers

# Import important libraries
import pandas as pd
from datasets import load_dataset
import transformers
from transformers import BlipProcessor, BlipForImageTextRetrieval,BlipForConditionalGeneration, AutoProcessor
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Resize
import os

import gc
import numpy as np
import pandas as pd
import itertools
from tqdm import tqdm
import albumentations as A
import cv2
import shutil
import json
from PIL import Image
import requests
from matplotlib import pyplot as plt

# Read CSV dataset from Pandas
df_train = pd.read_csv('/content/adr_train.csv') #, nrows = nRowsRead
df_train.dataframeName = 'adr_train.csv'
nRow, nCol = df_train.shape
print(f'There are {nRow} rows and {nCol} columns')

df_train.dataframeName

df_train.head()

filtered_df=df_train

filtered_df['image'] = "/content/drive/MyDrive/ADR_train" +"/"+ filtered_df['Image']
# filtered_df=filtered_df.drop(['images'], axis=1)

filtered_df.head()

# Create new directory for training images
folder_path = "/content/train"
if not os.path.exists(folder_path):
    os.mkdir(folder_path)

# Iterate through the DataFrame and move the files to the destination folder
for index, row in filtered_df.iterrows():
    source_file = row["image"]
    file_name = os.path.basename(source_file)
    destination_file = os.path.join(folder_path, file_name)

    # Use shutil.move() to move the file
    shutil.copy(source_file, folder_path)

# Delete extra column from the dataframe
filtered_df = filtered_df.drop(columns=["image"])

filtered_df.head()

# Convert dataframe to json format
captions = filtered_df.apply(lambda row: {"file_name": row["Image"], "text": row["Text"]}, axis=1).tolist()

# Save data to json file
with open(folder_path + "/metadata.jsonl", 'w') as f:
    for item in captions:
        f.write(json.dumps(item))

# Load dataset for training
dataset = load_dataset("imagefolder", data_dir=folder_path, split="train")
dataset

# Create class for training

class ImageCaptioningDataset(Dataset):
    def __init__(self, dataset, processor, image_size=(224, 224)):
        self.dataset = dataset
        self.processor = processor
        self.image_size = image_size
        self.resize_transform = Resize(image_size)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        encoding = self.processor(images=item["image"], text=item["text"], padding="max_length", return_tensors="pt")
        # remove batch dimension
        encoding = {k:v.squeeze() for k,v in encoding.items()}
        return encoding

# Load model from Huggingface Transformer library
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Create dataset for training
image_size = (224, 224)
train_dataset = ImageCaptioningDataset(dataset, processor, image_size)
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)
type(train_dataloader)

# initialize the optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.train()

# Start training
for epoch in range(20):
  print("Epoch:", epoch)
  for idx, batch in enumerate(train_dataloader):
    input_ids = batch.pop("input_ids").to(device)
    pixel_values = batch.pop("pixel_values").to(device)

    outputs = model(input_ids=input_ids,
                    pixel_values=pixel_values,
                    labels=input_ids)

    loss = outputs.loss

    #print("Loss:", loss.item())

    loss.backward()

    optimizer.step()
    optimizer.zero_grad()

  print("Loss:", loss.item())

import os

#### Create save model path and save the trained model
saved_folder_path = "/content/saved_model"
if not os.path.exists(saved_folder_path):
    os.mkdir(saved_folder_path)

model.save_pretrained(saved_folder_path)
processor.save_pretrained(saved_folder_path)

### Load the trained model
load_model = BlipForConditionalGeneration.from_pretrained(saved_folder_path)
load_processor = AutoProcessor.from_pretrained(saved_folder_path)

import os
from PIL import Image
import csv

# Folder containing your images
folder_path = "/content/ADR_test"

# Initialize an empty list to store image captions
captions = []

# Iterate through each image in the folder
for filename in os.listdir(folder_path):
    if filename.endswith((".jpg", ".jpeg")):
        # Open the image
        image_path = os.path.join(folder_path, filename)
        image = Image.open(image_path)

        # Prepare the image for the model
        inputs = processor(images=image, return_tensors="pt").to(device)
        pixel_values = inputs.pixel_values

        # Generate a caption
        generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

        # Append the filename and caption to the list
        captions.append((filename, generated_caption))

# Save the captions to a CSV file
output_csv = "image_captions.csv"
with open(output_csv, 'w', newline='') as csvfile:
    caption_writer = csv.writer(csvfile)
    caption_writer.writerow(["Image Filename", "Caption"])
    caption_writer.writerows(captions)

print(f"Captions saved to {output_csv}")

